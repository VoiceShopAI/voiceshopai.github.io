<!DOCTYPE html><html lang="en"> <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="keywords" content="Machine Learning, Audio Synthesis, Speech Synthesis, Speech-to-Speech, Zero-Shot, Disentangled Representation Learning, Diffusion Model, Flow-Based Generative Model, Seq2Seq, BN2BN, Voice Conversion, Accent Conversion, Speech Style Conversion, Age Conversion, Gender Conversion, SAMI, Speech Audio Music Intelligence, ByteDance, TikTok"> <meta name="description" lang="en" content="VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing"> <meta property="og:title" content="VoiceShop"> <meta property="og:site_name" content="VoiceShop"> <meta property="og:description" content="VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing"> <meta property="og:type" content="website"> <meta name="twitter:title" content="VoiceShop"> <meta name="twitter:description" content="VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing"> <link rel="icon" type="image/vnd.microsoft.icon" href="assets/icon/wave.png"> <link rel="stylesheet" href="css/style.css"> <title>VoiceShop</title> </head> <body> <!-- NAVIGATION --> <nav> <div id="menuToggle"> <input type="checkbox"/> <span></span> <span></span> <span></span> <ul id="menu"> <li><a class="links_nav" onclick="scrollToTop()">Home</a></li> <li><a class="links_nav" onclick="scrollToSection('abstract')">Abstract</a></li> <li><a class="links_nav" onclick="scrollToSection('method')">Method</a></li> <li><a class="links_nav" onclick="scrollToSection('zs_vc')">Zero-Shot Voice Conversion</a></li> <li><a class="links_nav" onclick="scrollToSection('bn2bn')">Accent and Speech Style Conversion</a></li> <li><a class="links_nav" onclick="scrollToSection('flow')">Age and Gender Editing</a></li> <li><a class="links_nav" onclick="scrollToSection('multi')">Combined Multi-Attribute Editing</a></li> <li><a class="links_nav" onclick="scrollToSection('ethics')">Ethical Considerations</a></li> <li><a class="links_nav" target="_blank" href="https://arxiv.org/abs/2404.06674">Paper <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M31 0H15v2h13.59L.29 30.29 1.7 31.7 30 3.41V16h2V1a1 1 0 0 0-1-1z"/></svg></a></li> </ul> </div> </nav> <!-- TITLE --> <h1 id="title-desktop">VoiceShop: A Unified Speech-to-Speech Framework<br>for Identity-Preserving Zero-Shot Voice Editing</h1> <h1 id="title-mobile">VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing</h1> <!-- AUTHOR LIST --> <div class="authors-desktop"> <a class="links" target="_blank" href="https://scholar.google.com/citations?hl=en&user=PE8HAfUAAAAJ">Philip Anastassiou</a>*&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=gPGVGTkAAAAJ&hl=en">Zhenyu Tang</a>*&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=Fhg8dSwAAAAJ&hl=en">Kainan Peng</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=o2ucdzsAAAAJ&hl=en">Dongya Jia</a><br> <a class="links"">Jiaxin Li</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=5BusdUwAAAAJ&hl=en">Ming Tu</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?hl=en&user=z60SFIwAAAAJ">Yuping Wang</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=3RaOfJkAAAAJ&hl=en">Yuxuan Wang</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=_vryxeMAAAAJ&hl=en">Mingbo Ma</a> </div> <div class="authors-mobile"> <a class="links" target="_blank" href="https://scholar.google.com/citations?hl=en&user=PE8HAfUAAAAJ">Philip Anastassiou</a>*&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=gPGVGTkAAAAJ&hl=en">Zhenyu Tang</a>*&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=Fhg8dSwAAAAJ&hl=en">Kainan Peng</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=o2ucdzsAAAAJ&hl=en">Dongya Jia</a>&emsp; <a class="links">Jiaxin Li</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=5BusdUwAAAAJ&hl=en">Ming Tu</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?hl=en&user=z60SFIwAAAAJ">Yuping Wang</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=3RaOfJkAAAAJ&hl=en">Yuxuan Wang</a>&emsp; <a class="links" target="_blank" href="https://scholar.google.com/citations?user=_vryxeMAAAAJ&hl=en">Mingbo Ma</a> </div> <!-- AFFILIATION --> <div class="organization"><sup>*</sup>Equal contribution&emsp;Data-Speech Team, ByteDance, San Jose, CA, USA</div> <br> <div class="centered">For easier navigation, please use the menu in the top-right corner.</div> <br> <!-- PAPER --> <div class="paper"><a class="links" target="_blank" href="https://arxiv.org/abs/2404.06674">Paper <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" style="width: 14px; height: 14px; vertical-align: middle;"><path d="M31 0H15v2h13.59L.29 30.29 1.7 31.7 30 3.41V16h2V1a1 1 0 0 0-1-1z"/></svg></a></div> <br> <!-- ABSTRACT --> <h2 id="abstract">Abstract</h2> <div class="abstract">We present VoiceShop, a novel speech-to-speech framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker's timbre. Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no zero-shot capability for out-of-distribution speakers, or the synthesized outputs exhibit undesirable timbre leakage. Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model finetuning.</div> <br> <br> <!-- METHOD --> <h2 id="method">Method</h2> VoiceShop is a speech foundation model capable of a wide assortment of zero-shot synthesis tasks, achieved through the use of a diffusion backbone model, which accepts global speaker embeddings and time-varying content features as conditioning signals, enabling robust zero-shot voice conversion.<br> <br> <div class="disclaimer">Click figures to view in full-screen.</div> <div class="zoom-container"><div class="figs"><img src="assets/figs/voiceshop.svg" onclick="openFullScreen(this)" alt="VoiceShop Model Design"></div></div> <br> We additionally train two separate task-specific editing modules based on a normalizing flow model that operates on the global speaker embeddings to achieve age and gender editing and a sequence-to-sequence model that operates on the local content features to achieve accent and speech style conversion.<br> <br> <!-- FLOW MODEL --> <div class="accordion"> <div class="accordion-section"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Attribute-Conditional Normalizing Flow for Age and Gender Editing</h4></div> <div class="accordion-content"> We observe that the speaker encoder jointly trained with our diffusion backbone model achieves strong speaker identity disentanglement, indicating that many attributes like age and gender are encoded into the speaker embedding vector. Therefore, the manipulation of these attributes can be seen as re-sampling from the learned latent space of speaker embeddings.<br> <div class="zoom-container"><div class="figs"><img src="assets/figs/flow.svg" onclick="openFullScreen(this)" alt="Flow Model Design"></div></div> To this end, we employ a continuous normalizing flow (CNF) model that operates on this latent space to achieve age and gender editing.<br> <div class="space"></div> </div> </div> </div> <!-- BN2BN MODEL --> <div class="accordion"> <div class="accordion-section"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Bottleneck-to-Bottleneck Conversion for Accent and Speech Style Conversion</h4></div> <div class="accordion-content"> We observe that time-varying content representations extracted from automatic speech recognition (ASR) models not only encode the semantics of speech signals (i.e., what is said), but also abundant pronunciation and prosody information (i.e., how it is said).<br> <br> <div class="zoom-container"><div class="figs"><img src="assets/figs/bn2bn_mono.svg" onclick="openFullScreen(this)" alt="BN2BN Model Design"></div></div> <br> Our BN2BN model maps the local content features of utterances from an arbitrary number of source accents to those of an arbitrary number of target accents in a single model using a multi-decoder architecture, effectively reducing the accent conversion task to a machine translation problem.<br> <div class="space"></div> </div> </div> </div> <br> <br> <!-- ZERO-SHOT VC --> <h2 id="zs_vc">Zero-Shot Voice Conversion</h2> VoiceShop is capable of monolingual and cross-lingual zero-shot voice conversion, enabling users to convert the timbre of utterances to arbitrary target speakers without additional model finetuning. All speakers listed below are not seen during training.<br> <br> <div class="figs"><img src="assets/figs/zsvc.svg" onclick="openFullScreen(this)" alt="Zero-Shot Voice Conversion"></div> <br> <!-- MONOLINGUAL VC --> <h3>Monolingual Zero-Shot Voice Conversion</h3> In monolingual voice conversion, both source and target speech are spoken in the same language.<br> <br> <!-- ENGLISH-TO-ENGLISH VC --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/en2en_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>English-to-English</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- MANDARIN-TO-MANDARIN VC --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/cn2cn_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Mandarin-to-Mandarin</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <!-- CROSS-LINGUAL VC --> <h3>Cross-Lingual Zero-Shot Voice Conversion</h3> Cross-lingual voice conversion extends the monolingual case by applying the target timbre of a speaker in one language to the spoken content of another speaker in a different language. We demonstrate applying the timbre of a Mandarin speaker to English content, and vice versa. Since the global speaker embedding predicted by our diffusion backbone model collapses temporal information, we also apply this process on out-of-distribution languages not seen during training, allowing anyone to speak fluent English or Mandarin in their own voice in a zero-shot manner.<br> <br> <!-- ENGLISH-TO-MANDARIN --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/en2cn_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>English-to-Mandarin</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- MANDARIN-TO-ENGLISH --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/cn2en_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Mandarin-to-English</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- OOD-TO-ENGLISH --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/ood2en_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Out-of-Distribution Languages-to-English</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- OOD-TO-MANDARIN --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/ood2cn_vc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Out-of-Distribution Languages-to-Mandarin</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <br> <!-- ACCENT CONVERSION --> <h2 id="bn2bn">Identity-Preserving Accent and Speech Style Conversion</h2> VoiceShop is capable of monolingual and cross-lingual identity-preserving zero-shot many-to-many accent and speech style conversion. This performance is achieved through the use of a bottleneck-to-bottleneck (BN2BN) module, which maps time-varying content features from an arbitrary number of source accents to those of an arbitrary number of target accents. All speakers listed below are not seen during training. Test samples are synthetically generated speech using publicly available Microsoft Azure TTS models in various accents.<br> <br> <div class="figs"><img src="assets/figs/ac.svg" onclick="openFullScreen(this)" alt="Accent and Speech Style Conversion"></div> <br> <!-- MONOLINGUAL AC --> <h3>Monolingual Accent Conversion</h2> In the monolingual case, accent conversion occurs within the same language.<br> <br> <!-- ENGLISH-TO-ENGLISH AC --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/en_ac.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>English-to-English</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- MANDARIN-TO-MANDARIN AC --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/cn_ac.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Mandarin-to-Mandarin</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <!-- CROSS-LINGUAL AC --> <h3>Cross-Lingual Accent Conversion</h3> In cross-lingual accent conversion, we convert source accents to those of a different language in the absence of parallel data (e.g., apply a British accent to Mandarin speech only using British-accented English speech or apply a Sichuan accent to English speech only using Sichuan-accented Mandarin speech, as recordings of British-accented Mandarin speech or Sichuan-accented English speech are not available).<br> <br> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/x_ac.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>English and Mandarin</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <!-- SPEECH STYLE CONVERSION --> <h3>Speech Style Conversion</h3> Beyond typical accent conversion, our BN2BN model is also capable of generalized speech style transfer. There are no specific requirements for what constitutes a "speech style," which may be as broad as emotional speech or the speaking styles of iconic personalities from popular culture.<br> <br> <!-- YOUTH --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/youth_sc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>"Sarcastic Youth" Speech Style Conversion</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- BRITISH --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/british_sc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>"Formal British" Speech Style Conversion</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- CARTOON --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/cartoon_sc.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>"Cartoon Character" Speech Style Conversion</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <br> <!-- AGE AND GENDER EDITING --> <h2 id="flow">Identity-Preserving Age and Gender Editing</h2> VoiceShop is capable of identity-preserving zero-shot age and gender editing. This performance is achieved through the use of an attribute-conditional normalizing flow model which operates on global speaker embeddings predicted by our diffusion backbone model. Conversion is performed in a continuous manner, allowing users to gradually interpolate across the spectrum of these attributes. All speakers in the following examples are not seen during training.<br> <br> <div class="figs"><img src="assets/figs/age.svg" onclick="openFullScreen(this)" alt="Age and Gender Editing"></div> <br> <!-- AGE EDITING --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/age.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Continuous Age Editing</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- GENDER EDITING --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/gender.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Continuous Gender Editing</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <br> <!-- COMBINED MULTI-ATTRIBUTE EDITING --> <h2 id="multi">Combined Multi-Attribute Editing</h2> VoiceShop is capable of combined multi-attribute editing, enabling arbitrary unseen speakers to simultaneously modify their accent, age, and gender in a single forward pass.<br> <br> <div class="figs"><img src="assets/figs/combined.svg" onclick="openFullScreen(this)" alt="Combined Multi-Attribute Editing"></div> <br> <!-- AUSTRALIAN SPEAKER --> <h3>Simultaneous Accent, Age, and Gender Editing</h3> We demonstrate VoiceShop's ability to simultaneously edit a speaker's accent, age, and gender in a zero-shot manner using two examples of Australian and British source speakers. Both speakers are not seen by the model during training.<br> <br> <h4>Australian Speaker</h4> We begin with a recording of an unseen out-of-domain speaker:<br> <br> <div style="text-align: center; margin: 0 auto;"> <audio src="https://philipanastassiou.com/voiceshop/audio_mp3/multi_attribute/australian_speaker/input.mp3" controls controlsList="nodownload novolume noplaybackrate" style="width: 50%;"></audio> </div> <br> We edit various attributes of this speaker's voice using our framework. We recommend referring back to this input sample occasionally to better understand the modifications enabled by the model.<br> <br> <!-- AUSTRALIAN SPEAKER: ONE ATTRIBUTE --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/aus_one.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit One Attribute</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- AUSTRALIAN SPEAKER: TWO ATTRIBUTES --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/aus_two.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit Two Attributes</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- AUSTRALIAN SPEAKER: THREE ATTRIBUTES --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/aus_three.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit Three Attributes</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <h4>British Speaker</h4> Let's consider another sample by a separate out-of-domain speaker:<br> <br> <div style="text-align: center; margin: 0 auto;"> <audio src="https://philipanastassiou.com/voiceshop/audio_mp3/multi_attribute/british_speaker/input.mp3" controls controlsList="nodownload novolume noplaybackrate" style="width: 50%;"></audio> </div> <br> As above, we recommend referring back to this input sample as you explore the model outputs below.<br> <br> <!-- BRITISH SPEAKER: ONE ATTRIBUTE --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/br_one.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit One Attribute</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- BRITISH SPEAKER: TWO ATTRIBUTES --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/br_two.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit Two Attributes</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <!-- BRITISH SPEAKER: THREE ATTRIBUTES --> <div class="accordion"> <div class="accordion-section" data-content-url="accordion/br_three.html"> <div class="accordion-header"><span class="accordion-arrow"></span><h4>Edit Three Attributes</h4></div> <div class="accordion-content"> <div class="placeholder"></div> </div> </div> </div> <br> <br> <!-- ETHICS --> <h2 id="ethics">Ethical Considerations</h2> As with all generative artificial intelligence systems, the real-world impact and potential for unintended misuse of models like VoiceShop must be considered. While there are beneficial use cases of our framework, such as providing entertainment value or lowering cross-cultural communication barriers by allowing users to speak other languages or accents in their own voice, its zero-shot capabilities could enable a user to generate misleading content with relative ease, such as synthesizing speech in the voice of an individual without their knowledge, presenting a risk of misinformation. In an effort to balance the need for transparent, reproducible, and socially responsible research practices, and due to the proprietary nature of portions of data used in this work, we share the details of our findings here, but do not plan to publicly release the model checkpoints or implementation at this time. The authors do not condone the use of this technology for illegal or malicious purposes. <!-- IMAGE OVERLAY --> <div id="overlay"> <span id="close-btn" onclick="closeFullScreen()">&times;</span> <img id="full-screen-img"> </div> <script src="js/script.js"></script> </body></html>